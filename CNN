def nvidia_model():
  # droupout layers has been removed because of over-fitting
  model = Sequential()
  model.add(Conv2D(24, kernel_size=(5,5), strides=(2,2), input_shape=(66, 200, 3), activation='elu'))
  model.add(Conv2D(36, kernel_size=(5,5), strides=(2,2), activation='elu'))
  model.add(Conv2D(48, kernel_size=(5,5), strides=(2,2), activation='elu'))
  model.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))
  model.add(Conv2D(64, kernel_size=(3, 3), activation='elu'))
  # model.add(Dropout(0.5))
  
  model.add(Flatten())
  model.add(Dense(100, activation='elu'))
  # model.add(Dropout(0.5))

  model.add(Dense(50, activation='elu'))
  # model.add(Dropout(0.5))

  model.add(Dense(10, activation ='elu'))
  # model.add(Dropout(0.5))
  
  model.add(Dense(1))

  optimizer = Adam(lr=1e-3)
  model.compile(loss='mse', optimizer=optimizer)
  return model
  
model = nvidia_model()
print(model.summary())

history = model.fit_generator(batch_generator(X_train, y_train, 64, 1),
                                  steps_per_epoch=500, 
                                  epochs=10,
                                  validation_data=batch_generator(X_valid, y_valid, 64, 0),
                                  validation_steps=200,
                                  verbose=1,
                                  shuffle=1)
#batch_size=100 means batch_generator will create new 100 images per step
#steps_per_epoch=300 means there will be 300 steps before completing an epoch
#which makes our system will have 30000 images per epoch  

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('loss')
plt.xlabel('# of epochs')

model.save("model.h5")
